import streamlit as st
import pandas as pd
import numpy as np
import re
import time
import os
from urllib.parse import quote
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
from bs4 import BeautifulSoup
import joblib
import matplotlib.pyplot as plt
from konlpy.tag import Okt
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tqdm import tqdm

# ==============================================================================
# ì¿¨ì•¤ì¡°ì´ í¬ë¡¤ëŸ¬ í•¨ìˆ˜
# ==============================================================================
def get_post_links(driver, search_query):
    """
    ê²€ìƒ‰ ê²°ê³¼ í˜ì´ì§€ì— ì ‘ì†í•˜ì—¬ ìƒìœ„ 5ê°œ ê²Œì‹œë¬¼ì˜ ë§í¬ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    """
    post_links = []
    try:
        encoded_query = quote(search_query)
        search_url = f"https://coolenjoy.net/bbs/search4.php?onetable=28&bo_table=28&sfl=wr_subject&stx={encoded_query}"
        
        driver.get(search_url)
        st.write(f"â„¹ï¸ ê²€ìƒ‰ ê²°ê³¼ í˜ì´ì§€ë¡œ ì´ë™: {search_url}")

        list_container_selector = "ul.na-table"
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.CSS_SELECTOR, list_container_selector))
        )
        
        soup = BeautifulSoup(driver.page_source, 'lxml')
        search_list_container = soup.select_one(list_container_selector)
        
        if not search_list_container:
            st.warning("âš ï¸ ê²€ìƒ‰ ê²°ê³¼ ì»¨í…Œì´ë„ˆë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return []

        link_tags = search_list_container.select('div.na-item a')
        
        for tag in link_tags:
            href = tag.get('href')
            if href and 'wr_id' in href:
                match = re.search(r'wr_id=(\d+)', href)
                if match:
                    wr_id = match.group(1)
                    post_url = f"https://coolenjoy.net/bbs/28/{wr_id}"
                    if post_url not in post_links:
                        post_links.append(post_url)
        
        post_links = post_links[:5]
        st.write(f"â„¹ï¸ ì´ {len(post_links)}ê°œì˜ ê²Œì‹œë¬¼ ë§í¬ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
        
    except Exception as e:
        st.error(f"âŒ ê²Œì‹œë¬¼ ë§í¬ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")

    return post_links

def get_text_from_post(driver, url):
    """
    ê°œë³„ ê²Œì‹œë¬¼ í˜ì´ì§€ì— ì ‘ì†í•˜ì—¬ ë³¸ë¬¸ê³¼ ëŒ“ê¸€ í…ìŠ¤íŠ¸ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    """
    texts = []
    try:
        driver.get(url)
        
        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID, "bo_v_atc")))
        soup = BeautifulSoup(driver.page_source, 'lxml')

        content_element = soup.find('div', id='bo_v_atc')
        if content_element:
            view_content = content_element.find('div', class_='view-content')
            if view_content:
                content_text = view_content.get_text(separator='\n', strip=True)
                texts.append({'text': content_text})
        
        comment_elements = soup.select('#bo_vc div.cmt_contents')
        for comment in comment_elements:
            secret_icon = comment.find('span', class_='na-icon na-secret')
            if secret_icon:
                continue

            comment_text = comment.get_text(separator='\n', strip=True)
            cleaned_text = re.sub(r'^@\S+\s+ë‹µê¸€\s*', '', comment_text.strip())

            if cleaned_text:
                texts.append({'text': cleaned_text})
                
    except Exception as e:
        st.warning(f"í˜ì´ì§€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {url} ({e})")
        
    return texts

# ==============================================================================
# ë°ì´í„° ì „ì²˜ë¦¬ ë° ê°ì„± ë¶„ì„ í•¨ìˆ˜
# ==============================================================================
def preprocess_texts(df):
    """
    ëª¨ë¸ í•™ìŠµ ì‹œì™€ ë™ì¼í•œ ë°©ë²•ìœ¼ë¡œ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì „ì²˜ë¦¬í•©ë‹ˆë‹¤.
    """
    # 1. í•œê¸€ ë° ê³µë°± ì œì™¸ ëª¨ë“  ë¬¸ì ì œê±°
    df['text'] = df['text'].str.replace("[^ã„±-ã…ã…-ã…£ê°€-í£ ]", "", regex=True)
    # 2. ë¹ˆ ê°’(empty string)ì„ NaNìœ¼ë¡œ ë³€ê²½ í›„ ì œê±°
    df['text'].replace('', np.nan, inplace=True)
    df.dropna(subset=['text'], inplace=True)
    
    # 3. í† í°í™” ë° ë¶ˆìš©ì–´ ì œê±°
    okt = Okt()
    stopwords = ['ì˜','ê°€','ì´','ì€','ë“¤','ëŠ”','ì¢€','ì˜','ê±','ê³¼','ë„','ë¥¼','ìœ¼ë¡œ','ì','ì—','ì™€','í•œ','í•˜ë‹¤','ë˜ë‹¤','ìˆë‹¤','ì´ë‹¤','ê·¸','ì €','ê²ƒ']
    
    tokenized_data = []
    for sentence in tqdm(df['text']):
        tokenized = okt.morphs(str(sentence), stem=True)
        stopwords_removed = [word for word in tokenized if not word in stopwords]
        tokenized_data.append(stopwords_removed)
        
    df['tokens'] = tokenized_data
    return df

def predict_sentiment(texts, model, tokenizer, max_len=100):
    """
    ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥ë°›ì•„ ê°ì„±ì„ ì˜ˆì¸¡í•©ë‹ˆë‹¤.
    """
    # ì˜ˆì¸¡ì„ ìœ„í•´ í† í°í™”ëœ ë¦¬ìŠ¤íŠ¸ë¥¼ ë‹¤ì‹œ ë¬¸ìì—´ë¡œ í•©ì¹¨
    sequences = tokenizer.texts_to_sequences(texts.apply(lambda x: ' '.join(x)))
    X = pad_sequences(sequences, maxlen=max_len, padding='pre')
    preds = model.predict(X, verbose=0)
    labels = ['ê¸ì •', 'ë¶€ì •', 'ì¤‘ë¦½']
    results = [labels[np.argmax(p)] for p in preds]
    probs = [float(np.max(p)) * 100 for p in preds]
    return results, probs

# ==============================================================================
# Streamlit UI ì„¤ì •
# ==============================================================================
plt.rcParams['font.family'] = 'Malgun Gothic'
plt.rcParams['axes.unicode_minus'] = False

st.set_page_config(page_title="GPU ì»¤ë®¤ë‹ˆí‹° ì—¬ë¡  ë¶„ì„", layout="wide")
st.title("ê·¸ë˜í”½ì¹´ë“œ ì»¤ë®¤ë‹ˆí‹° ì—¬ë¡  ë¶„ì„ ëŒ€ì‹œë³´ë“œ")

st.sidebar.title("ğŸ“Š í”„ë¡œì íŠ¸ ì†Œê°œ")
st.sidebar.markdown("ì¿¨ì•¤ì¡°ì´ ê·¸ë˜í”½ì¹´ë“œ ê²Œì‹œíŒì˜ ìµœì‹  ì—¬ë¡ ì„ ìˆ˜ì§‘í•˜ê³  ê°ì„± ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.")
st.sidebar.info("ë¶„ì„í•˜ê³  ì‹¶ì€ ê·¸ë˜í”½ì¹´ë“œ ëª¨ë¸ëª…ì„ ì…ë ¥í•˜ê³  ë¶„ì„ ì‹œì‘ ë²„íŠ¼ì„ ëˆ„ë¥´ì„¸ìš”.")

model_path = "./model/model.h5"
tokenizer_path = "./model/tokenizer.pickle"

@st.cache_resource
def load_models():
    if not os.path.exists(model_path) or not os.path.exists(tokenizer_path):
        return None, None
    model = load_model(model_path)
    tokenizer = joblib.load(tokenizer_path)
    return model, tokenizer

model, tokenizer = load_models()

if not model or not tokenizer:
    st.error("â— ëª¨ë¸ ë˜ëŠ” í† í¬ë‚˜ì´ì € íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. './model/' í´ë”ì— íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
else:
    query = st.text_input("ğŸ” ë¶„ì„í•  ê·¸ë˜í”½ì¹´ë“œ ëª¨ë¸ëª…ì„ ì…ë ¥í•˜ì„¸ìš”", placeholder="ì˜ˆ: RTX 5080")

    if st.button("ğŸš€ ì»¤ë®¤ë‹ˆí‹° ì—¬ë¡  ìˆ˜ì§‘ ë° ê°ì„± ë¶„ì„ ì‹œì‘"):
        if not query.strip():
            st.warning("â— ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
        else:
            all_texts = []
            chrome_options = Options()
            chrome_options.add_argument("--headless")
            chrome_options.add_argument("--no-sandbox")
            chrome_options.add_argument("--disable-dev-shm-usage")
            chrome_options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36")
            chrome_options.add_argument('--log-level=3')
            
            driver = None
            try:
                with st.spinner("1ï¸âƒ£ ì»¤ë®¤ë‹ˆí‹°ì—ì„œ ìµœì‹  ê²Œì‹œë¬¼ì„ ìˆ˜ì§‘í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                    driver = webdriver.Chrome(options=chrome_options)
                    post_links = get_post_links(driver, query)
                    
                    if not post_links:
                        st.warning("ìˆ˜ì§‘í•  ê²Œì‹œë¬¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                        st.stop()

                    for i, link in enumerate(post_links):
                        st.write(f"   - [{i+1}/{len(post_links)}] '{link}' ì²˜ë¦¬ ì¤‘...")
                        texts_from_post = get_text_from_post(driver, link)
                        if texts_from_post:
                            all_texts.extend(texts_from_post)
                        time.sleep(1)

                st.success(f"âœ… ê²Œì‹œë¬¼ ìˆ˜ì§‘ ì™„ë£Œ! ì´ {len(all_texts)}ê°œì˜ í…ìŠ¤íŠ¸(ë³¸ë¬¸+ëŒ“ê¸€)ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")

                if not all_texts:
                    st.warning("ìˆ˜ì§‘ëœ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤. ë¶„ì„ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                    st.stop()
                
                df_crawled = pd.DataFrame(all_texts)
                
                with st.spinner("2ï¸âƒ£ í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„ ê°€ëŠ¥í•œ í˜•íƒœë¡œ ì „ì²˜ë¦¬í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                    df_processed = preprocess_texts(df_crawled)
                st.success("âœ… í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ ì™„ë£Œ!")

                with st.spinner("3ï¸âƒ£ AI ëª¨ë¸ë¡œ ê°ì„±ì„ ì˜ˆì¸¡í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                    preds, probs = predict_sentiment(df_processed['tokens'], model, tokenizer)
                    df_processed['ê°ì„±'] = preds
                    df_processed['ì‹ ë¢°ë„(%)'] = probs
                st.success("âœ… ê°ì„± ë¶„ì„ ì™„ë£Œ!")

                st.subheader(f"'{query}'ì— ëŒ€í•œ ì»¤ë®¤ë‹ˆí‹° ì—¬ë¡  ë¶„ì„ ê²°ê³¼")
                col1, col2 = st.columns([1, 1])

                with col1:
                    st.markdown("#### ğŸ“Š ê°ì„± ë¹„ìœ¨")
                    fig, ax = plt.subplots(figsize=(6, 5))
                    value_counts = df_processed['ê°ì„±'].value_counts().reindex(['ê¸ì •', 'ì¤‘ë¦½', 'ë¶€ì •']).fillna(0)
                    colors = ['#2ECC71', '#BDC3C7', '#E74C3C']
                    ax.pie(value_counts, labels=value_counts.index, autopct='%1.1f%%', startangle=90, colors=colors, textprops={'fontsize': 12})
                    ax.axis('equal')
                    st.pyplot(fig)

                with col2:
                    st.markdown("#### ğŸ”¢ ê°ì„±ë³„ í…ìŠ¤íŠ¸ ê°œìˆ˜")
                    st.markdown(f"##### ğŸŸ¢ ê¸ì •: {int(value_counts.get('ê¸ì •', 0))}ê°œ")
                    st.markdown(f"##### âšª ì¤‘ë¦½: {int(value_counts.get('ì¤‘ë¦½', 0))}ê°œ")
                    st.markdown(f"##### ğŸ”´ ë¶€ì •: {int(value_counts.get('ë¶€ì •', 0))}ê°œ")

                st.markdown("---")
                st.subheader("ğŸ“ ì„¸ë¶€ ë¶„ì„ ê²°ê³¼")
                st.dataframe(df_processed[['text', 'ê°ì„±', 'ì‹ ë¢°ë„(%)']].rename(columns={'text': 'ì›ë¬¸'}))

            except Exception as e:
                st.error(f"âŒ ì „ì²´ ì‘ì—… ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            finally:
                if driver:
                    driver.quit()

#딥러닝 기반 한국어 리뷰 감성 분석 모델📌 프로젝트 개요본 프로젝트는 한국어 텍스트(리뷰, 댓글 등)의 감성(긍정, 부정, 중립)을 효과적으로 분류하는 딥러닝 모델을 개발하는 것을 목표로 합니다.학습된 모델과 전처리기를 저장하여, 새로운 문장이 주어졌을 때 실시간으로 감성을 예측하는 애플리케이션에 쉽게 적용할 수 있습니다.✨ 주요 특징대규모 데이터 기반 학습: 60만 개 이상의 레이블링된 대규모 데이터를 사용하여 모델의 일반화 성능을 높였습니다.안정적인 데이터 처리: Chunk 단위 저장 및 진행 상황 기록을 통해 대용량 데이터 처리 중 예기치 않은 중단이 발생하더라도 안전하게 작업을 재개할 수 있습니다.고성능 딥러닝 모델: 양방향 LSTM(Bidirectional LSTM) 모델을 기반으로 문맥을 깊이 있게 이해하여 높은 정확도의 감성 분류를 수행합니다.모듈화된 예측 기능: 학습된 모델과 전처리기를 별도로 저장하여, 새로운 텍스트에 대한 감성 예측 기능을 쉽게 재사용할 수 있도록 구현했습니다.📂 프로젝트 구조.
```
├── data/
│   └── labeld.csv            # (필수) 학습에 사용될 레이블링된 데이터
├── 01_data_labeling.py       # (선택) 원본 데이터에 LLM으로 감성 레이블을 추가하는 스크립트
├── 02_train_model.py         # (필수) 데이터 전처리 및 감성 분석 모델 학습 스크립트
├── 03_predict_sentiment.py   # (필수) 저장된 모델과 토크나이저를 사용해 새로운 문장을 예측하는 스크립트
├── best_sentiment_model.h5   # 학습 완료 후 생성되는 최고 성능의 모델 파일
├── tokenizer.pickle          # 학습 완료 후 생성되는 단어-인덱스 사전 파일
└── README.md                 # 프로젝트 설명 파일
```
🚀 시작하기1. 환경 설정라이브러리 설치먼저 프로젝트에 필요한 라이브러리들을 설치합니다.pip install pandas numpy scikit-learn tensorflow konlpy tqdm matplotlib seaborn
하드웨어 권장 사양학습(Training): 대규모 데이터셋으로 모델을 빠르게 학습시키기 위해 NVIDIA 그래픽카드(GPU) 사용을 권장합니다.예측(Inference): CPU 환경에서도 충분히 동작합니다.2. 데이터 준비data/labeld.csv 파일을 준비합니다. 파일은 최소 text와 label 컬럼을 포함해야 합니다.textlabel이 영화 정말 재미있었어요!긍정음식이 너무 짜고 맛이 없어요부정그냥 그저 그랬어요중립3. 모델 학습 실행아래 명령어를 실행하여 데이터 전처리, 모델 학습 및 저장을 한 번에 수행합니다. 학습이 완료되면 best_sentiment_model.h5와 tokenizer.pickle 파일이 생성됩니다.python 02_train_model.py
학습 과정 중 검증 데이터셋에 대해 가장 높은 정확도(val_accuracy)를 기록하는 시점의 모델이 best_sentiment_model.h5로 자동 저장됩니다.4. 새로운 문장으로 감성 예측하기학습된 모델을 사용하여 새로운 문장의 감성을 예측하려면 아래 스크립트를 실행하세요.python 03_predict_sentiment.py
스크립트를 실행하면 내장된 예시 문장들에 대한 예측 결과가 다음과 같이 출력됩니다.감성 분석 모델 및 전처리기 로딩 중...
✅ 모든 객체 로딩 완료!
========================================
입력 문장: "와 이 영화 진짜 핵노잼 ㅋㅋ 다시는 안봄"
▶ 예측 결과: 부정 (98.72%)
--- 클래스별 확률 ---
  - 긍정: 0.51%
  - 부정: 98.72%
  - 중립: 0.77%
------------------------------
🤖 모델 아키텍처본 프로젝트의 감성 분석 모델은 다음과 같은 구조로 설계되었습니다.Embedding Layer: 단어를 고차원의 벡터로 변환 (Embedding Dim: 200)Bidirectional LSTM Layer: 문장의 순방향 및 역방향 문맥 정보를 모두 학습 (Hidden Units: 128)Bidirectional LSTM Layer: 더 깊은 수준의 문맥 정보 추출 (Hidden Units: 128)Dropout Layer: 학습 과정에서 일부 뉴런을 비활성화하여 과적합 방지Dense Layer (ReLU): 비선형성을 추가하여 모델의 표현력 강화Dropout Layer: 추가적인 과적합 방지Dense Layer (Softmax): 최종적으로 '긍정', '부정', '중립' 각 클래스에 대한 확률을 출력
